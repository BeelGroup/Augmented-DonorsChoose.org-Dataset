log_level: 10
donations_filepath: data/donorschoose.org/Donations.csv
projects_filepath: data/donorschoose.org/Projects.csv
random_state_seed: 2718281828
# Apply cleaning methods and sample the data as to reduce the amount of required memory
sampling_methods:
    drop_raw_values:
        - '0. <= DonationAmount <= 2.'
    frequency_boundaries:
        # Conditions must not be met at the same time; though, the last one will always hold
        - ['ProjectID', 2]
        - ['DonorID', 2]
    sample: 10000
n_folds: 5
algorithms_args:
    SciPy-SVD:
        n_components: 100
    SKLearn-SVD:
        n_components: 100
    SKLearn-KNN:
        n_neighbors: 40
    SKLearn-NMF:
        n_components: 50
    SPL-SVD: {}
    SPL-SVDpp: {}
    SPL-NMF: {}
    SPL-KNNWithMeans:
        verbose: False
    SPL-KNNBasic:
        verbose: False
    SPL-KNNWithZScore:
        verbose: False
    SPL-KNNBaseline:
        verbose: False
    SPL-NormalPredictor: {}
    SPL-CoClustering: {}
    SPL-SlopeOne: {}
rating_scores: [1, 2, 3, 4, 5]
# Cut only on the given quantile range to mitigate the effect of outliers and append the bottom and top to the first respectively last bin afterwards
rating_range_quantile: [0., 0.99]
