log_level: 10
donations_filepath: data/donorschoose.org/Donations.csv
projects_filepath: data/donorschoose.org/Projects.csv
random_state_seed: 2718281828
# Constant for the top-N accuracy metrics
n_random_non_interacted_items: 100
# Apply cleaning methods and sample the data as to reduce the amount of required memory
sampling_methods:
    drop_raw_values:
        - '0. <= DonationAmount <= 2.'
    frequency_boundaries:
        # Conditions must not be met at the same time; though, the last one will always hold
        - ['DonorID', 2]
    sample: 10000
n_folds: 5
algorithms_args:
    SciPy-SVD:
        n_components: 100
    SKLearn-SVD:
        n_components: 100
    SKLearn-KNN:
        n_neighbors: 40
    SKLearn-NMF:
        n_components: 50
    SPL-SVD: {}
    SPL-SVDpp: {}
    SPL-NMF: {}
    SPL-KNNWithMeans:
        verbose: False
    SPL-KNNBasic:
        verbose: False
    SPL-KNNWithZScore:
        verbose: False
    SPL-KNNBaseline:
        verbose: False
    SPL-NormalPredictor: {}
    SPL-CoClustering: {}
    SPL-SlopeOne: {}
    SKLearn-TfidfVectorizer:
        analyzer: 'word'
        ngram_range: [1, 2]
        stop_words: 'english'
        lowercase: True
        max_df: 0.9
        max_features: 5000
accuracy_methods:
    - 'RMSE'
    - 'MAE'
    - 'RecallAtPosition'
rating_scores: [1, 2, 3, 4, 5]
# Cut only on the given quantile range to mitigate the effect of outliers and append the bottom and top to the first respectively last bin afterwards
rating_range_quantile: [0., 0.99]
